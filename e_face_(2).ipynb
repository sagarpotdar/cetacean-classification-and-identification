{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "e-face (2).ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.368566Z",
          "iopub.execute_input": "2022-06-06T13:09:17.368982Z",
          "iopub.status.idle": "2022-06-06T13:09:17.390313Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.368874Z",
          "shell.execute_reply": "2022-06-06T13:09:17.389486Z"
        },
        "trusted": true,
        "id": "PXfPTP9xy2xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jSyRsqAKy2xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# is_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
        "# if is_kaggle:\n",
        "#     print('Running in Kaggle Kernels')\n",
        "#     from kaggle_secrets import UserSecretsClient\n",
        "#     user_secrets = UserSecretsClient()\n",
        "#     wandb_creds = user_secrets.get_secret(\"wandb\")\n",
        "    \n",
        "#     !pip3 install -Uq tensorflow==2.7\n",
        "#     print(\"update TPU server tensorflow version…\")\n",
        "\n",
        "#     !pip install -q cloud-tpu-client\n",
        "#     import tensorflow as tf\n",
        "#     from cloud_tpu_client import Client\n",
        "#     print(tf.version)\n",
        "#     Client().configure_tpu_version(2.7, restart_type='ifNeeded')\n",
        "\n",
        "#     !pip install -Uq tensorflow-gcs-config==2.7\n",
        "\n",
        "#     !pip install wandb > /dev/null\n",
        "#     !pip install -q efficientnet > /dev/null\n",
        "#     !pip install tensorflow_addons > /dev/null\n",
        "    \n",
        "    \n",
        "#     !wandb login {wandb_creds}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.391686Z",
          "iopub.execute_input": "2022-06-06T13:09:17.39205Z",
          "iopub.status.idle": "2022-06-06T13:09:17.681015Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.392023Z",
          "shell.execute_reply": "2022-06-06T13:09:17.67909Z"
        },
        "trusted": true,
        "id": "r9OzFy3hy2xe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "35a97566-8868-4c45-fda3-7f7f4118046c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a86619d4d7ca>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print('Running in Kaggle Kernels')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq tensorflow-gcs-config==2.7\n",
        "\n",
        "!pip install wandb > /dev/null\n",
        "!pip install -q efficientnet > /dev/null\n",
        "!pip install tensorflow_addons > /dev/null\n",
        "!pip3 install -Uq tensorflow==2.7    \n",
        "!pip install -q cloud-tpu-client\n",
        "    \n",
        "!wandb login {wandb_creds}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Running in Kaggle Kernels')\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# wandb_creds = user_secrets.get_secret(\"wandb\")\n",
        "    \n",
        "#!pip3 install -Uq tensorflow==2.7\n",
        "#print(\"update TPU server tensorflow version…\")\n",
        "#!pip install -q cloud-tpu-client\n",
        "#import tensorflow as tf\n",
        "from cloud_tpu_client import Client\n",
        "print(tf.version)\n",
        "Client().configure_tpu_version(2.7, restart_type='ifNeeded')\n",
        "\n",
        "# !pip install -Uq tensorflow-gcs-config==2.7\n",
        "\n",
        "# !pip install wandb > /dev/null\n",
        "# !pip install -q efficientnet > /dev/null\n",
        "# !pip install tensorflow_addons > /dev/null\n",
        "    \n",
        "    \n",
        "# !wandb login {wandb_creds}"
      ],
      "metadata": {
        "id": "_XOyj0yby2xf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "476d3510-8891-433b-cf81-ffcad7d58b85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/wandb\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/cli/cli.py\", line 95, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/cli/cli.py\", line 243, in login\n",
            "    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 76, in login\n",
            "    configured = _login(**kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 291, in _login\n",
            "    wlogin.configure_api_key(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 175, in configure_api_key\n",
            "    apikey.write_key(self._settings, key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/apikey.py\", line 210, in write_key\n",
            "    raise ValueError(\"API key must be 40 characters long, yours was %s\" % len(key))\n",
            "ValueError: API key must be 40 characters long, yours was 13\n",
            "Running in Kaggle Kernels\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DuplicateFlagError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-adee66f2f402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#!pip install -q cloud-tpu-client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#import tensorflow as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcloud_tpu_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_tpu_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestart_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ifNeeded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cloud_tpu_client/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcloud_tpu_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cloud_tpu_client/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m flags.DEFINE_bool('runtime_oom_exit', True,\n\u001b[0;32m---> 43\u001b[0;31m                   'Exit the script when the TPU runtime is OOM.')\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0m_GKE_ENV_VARIABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_boolean\u001b[0;34m(name, default, help, flag_values, module_name, required, **args)\u001b[0m\n\u001b[1;32m    330\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    331\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBooleanFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m       required)\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'runtime_oom_exit' is defined twice. First from tensorflow.python.tpu.client.client, Second from cloud_tpu_client.client.  Description from first occurrence: Exit the script when the TPU runtime is OOM."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q efficientnet\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "!pip install wandb\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import efficientnet.tfkeras as efn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.681838Z",
          "iopub.status.idle": "2022-06-06T13:09:17.682709Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.682517Z",
          "shell.execute_reply": "2022-06-06T13:09:17.682537Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__k_cbvfy2xg",
        "outputId": "6badb118-17c7-4c60-f68b-8250944f7c8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 75.0 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 76.0 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=cd5085b34746e61b1591dc3a20dbc4b27f08c7f0fdde3123b941d290e552b1cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tOqwdTAOy2xg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ENAezJXhAu8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_datasets import KaggleDatasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "4Pi0RI6FAu5K",
        "outputId": "96feb265-69f9-49ff-f46c-6a23418e3b7b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bb97e2066673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKaggleDatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "47VOWdMnAu2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_strategy(tpu_arg):\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_arg)\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError as e:\n",
        "        print(f'No TPU: {e}')\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    else:\n",
        "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        \n",
        "    return strategy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.683687Z",
          "iopub.status.idle": "2022-06-06T13:09:17.684209Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.683987Z",
          "shell.execute_reply": "2022-06-06T13:09:17.684012Z"
        },
        "trusted": true,
        "id": "NXacbmJXy2xg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpu_arg = None\n",
        "# if not is_kaggle:\n",
        "#     tpu_arg = 'local'\n",
        "    \n",
        "strategy = get_strategy(tpu_arg)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.685206Z",
          "iopub.status.idle": "2022-06-06T13:09:17.685713Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.685478Z",
          "shell.execute_reply": "2022-06-06T13:09:17.685503Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TZaOPTy2xh",
        "outputId": "3cf0e223-5a42-4224-b08b-6525bf5cb889"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.106.207.178:8470\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.207.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.207.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Uvx5XCN7y2xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    \n",
        "    SEED = 420\n",
        "    # Trained on the full dataset.\n",
        "    FOLD_TO_RUN = None\n",
        "    VAL_FOLD_TO_RUN = 0\n",
        "    FOLDS = 5\n",
        "    DEBUG = False\n",
        "    EVALUATE = True\n",
        "    RESUME = False\n",
        "    RESUME_EPOCH = None\n",
        "\n",
        "    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    IMAGE_SIZE = 864\n",
        "    N_CLASSES = 15587\n",
        "\n",
        "    model_type = 'effnetv1'  \n",
        "    EFF_NET = 5\n",
        "\n",
        "    head = 'arcface'\n",
        "\n",
        "    EPOCHS = 30\n",
        "    EPOCHS_STAGE_2 = 0\n",
        "\n",
        "    LR = 0.001\n",
        "\n",
        "    RGB_TO_GRAY_PROB = 0.15\n",
        "\n",
        "    WEIGHT_METRIC = 0.5\n",
        "    WEIGHT_SPECIES = 0.5\n",
        "    \n",
        "    save_dir = './output'\n",
        "    \n",
        "    KNN = 100\n",
        "    \n",
        "    MIN_JPEG_QUAL_AUG = 75\n",
        "    MAX_JPEG_QUAL_AUG = 100\n",
        "    \n",
        "    FULLBODY_CROP_PROB = 0.2\n",
        "\n",
        "    DETIC_CROP_PROB = 0.14\n",
        "    YOLOV5_CROP_PROB = 0.14\n",
        "    \n",
        "    VIT_BASE_CROP_PROB = 0.14\n",
        "    VIT_SMALL_CROP_PROB = 0.14\n",
        "    VIT_MOCO_CROP_PROB = 0.14\n",
        "    \n",
        "    RANDOM_HUE_MAX_DELTA = 0.1\n",
        "\n",
        "    RANDOM_SATURATION_LOWER = 0.75\n",
        "    RANDOM_SATURATION_UPPER = 1.25\n",
        "    \n",
        "    RANDOM_CONTRAST_LOWER = 0.75\n",
        "    RANDOM_CONTRAST_UPPER = 1.25\n",
        "    \n",
        "    RANDOM_BRIGHTNESS_MAX_DELTA = 0.1\n",
        "\n",
        "    NONE_CROP_PROB = 0.1\n",
        "    \n",
        "    ARC_FACE_M = 0.3\n",
        "    \n",
        "    EMB_DIM = 1024\n",
        "\n",
        "    LOAD_WEIGHTS_STAGE_2 = None\n",
        "    LOAD_WEIGHTS = None\n",
        "    INF_LOAD_WEIGHTS = None\n",
        "    \n",
        "    GCS_DS_PATH = 'happywhale-tfrecords-private2'\n",
        "    GCS_PSEUDO_DS_PATH = 'happywhale-tfrecords-pseudo-dhak'\n",
        "    \n",
        "    INPUT_DATA_PATH = Path('../input/happy-whale-and-dolphin/')\n",
        "    LABEL_INFO_PATH = Path('../input/happywhale-generate-tfrecords-with-pseudo/')\n",
        "    \n",
        "    GCS_UPLOAD_BUCKET = None\n",
        "    \n",
        "    CROP_WEIGHTS = {\n",
        "        'fullbody': 10,\n",
        "        'detic': 1,\n",
        "        'yolov5': 1,\n",
        "        'vit_base': 1,\n",
        "        'vit_small': 1,\n",
        "        'vit_moco': 1,\n",
        "        'no_crop': 2,\n",
        "    }\n",
        "\n",
        "    \n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "         for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def is_interactive():\n",
        "    return 'runtime'    in get_ipython().config.IPKernelApp.connection_file\n",
        "IS_INTERACTIVE = is_interactive()\n",
        "print(IS_INTERACTIVE)\n",
        "\n",
        "MODEL_NAME = None\n",
        "if config.model_type == 'effnetv1':\n",
        "    MODEL_NAME = f'effnetv1_b{config.EFF_NET}'\n",
        "elif config.model_type == 'effnetv2':\n",
        "    MODEL_NAME = f'effnetv2_{config.EFF_NETV2}'\n",
        "\n",
        "config.MODEL_NAME = MODEL_NAME\n",
        "print(MODEL_NAME)\n",
        "\n",
        "# if is_kaggle:\n",
        "#     print('Loading GCS path from KaggleDatasets')\n",
        "#     from kaggle_secrets import UserSecretsClient\n",
        "#     from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "#     user_secrets = UserSecretsClient()\n",
        "#     user_credential = user_secrets.get_gcloud_credential()\n",
        "#     user_secrets.set_tensorflow_credential(user_credential)\n",
        "\n",
        "#     config.GCS_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_DS_PATH)\n",
        "#     if config.GCS_PSEUDO_DS_PATH:\n",
        "#         config.GCS_PSEUDO_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_PSEUDO_DS_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.686709Z",
          "iopub.status.idle": "2022-06-06T13:09:17.6872Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.686983Z",
          "shell.execute_reply": "2022-06-06T13:09:17.687008Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r2DmVhmy2xi",
        "outputId": "78ee3cf5-ac2e-454e-c468-5a7528a06145"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "effnetv1_b5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.GCS_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_DS_PATH)\n",
        "#     if config.GCS_PSEUDO_DS_PATH:\n",
        "#         config.GCS_PSEUDO_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_PSEUDO_DS_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "TazyXATkAZhm",
        "outputId": "62e901ea-d9d9-4f80-e103-919c9986de52"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-567745960baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCS_DS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gcs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCS_DS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#     if config.GCS_PSEUDO_DS_PATH:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#         config.GCS_PSEUDO_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_PSEUDO_DS_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KaggleDatasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.GCS_PSEUDO_DS_PATH = KaggleDatasets().get_gcs_path(config.GCS_PSEUDO_DS_PATH)"
      ],
      "metadata": {
        "id": "HudcZcp-AZQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yGdzPXmfAZNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4rEYpziqAZLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_-ogOUyRy2xj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Doing fold {config.FOLD_TO_RUN} (total fold: {config.FOLDS})')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.688372Z",
          "iopub.status.idle": "2022-06-06T13:09:17.688862Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.688643Z",
          "shell.execute_reply": "2022-06-06T13:09:17.68867Z"
        },
        "trusted": true,
        "id": "JyVQ6Fs9y2xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845d5cbb-b92f-446e-c5c0-67a16d062ce1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing fold None (total fold: 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = Path(config.save_dir) / datetime.now().strftime('%Y%m%d-%H%M%S')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.690121Z",
          "iopub.status.idle": "2022-06-06T13:09:17.690481Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.690273Z",
          "shell.execute_reply": "2022-06-06T13:09:17.690302Z"
        },
        "trusted": true,
        "id": "qQ1cXvEny2xj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(OUTPUT_DIR)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.691559Z",
          "iopub.status.idle": "2022-06-06T13:09:17.691979Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.691783Z",
          "shell.execute_reply": "2022-06-06T13:09:17.691806Z"
        },
        "trusted": true,
        "id": "PIwNp4SAy2xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7593c51-e926-429d-f3ad-f9fa12c00ee9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output/20220609-101546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.693062Z",
          "iopub.status.idle": "2022-06-06T13:09:17.693396Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.693213Z",
          "shell.execute_reply": "2022-06-06T13:09:17.693235Z"
        },
        "trusted": true,
        "id": "NFGuHgc1y2xk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if is_kaggle == 'Interactive':\n",
        "#     print('Wandb in offline mode.')\n",
        "#     os.environ['WANDB_MODE'] = 'offline'\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.694542Z",
          "iopub.status.idle": "2022-06-06T13:09:17.694875Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.694691Z",
          "shell.execute_reply": "2022-06-06T13:09:17.694715Z"
        },
        "trusted": true,
        "id": "nBeUktuPy2xk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config = {\n",
        "    'seed': config.SEED,\n",
        "    'image_size': config.IMAGE_SIZE,\n",
        "    'model_type': config.model_type,\n",
        "    'lr': config.LR,\n",
        "    'epochs': config.EPOCHS,\n",
        "    'knn': 100,\n",
        "    'metric_out': config.WEIGHT_METRIC,\n",
        "    'specie_out': config.WEIGHT_SPECIES,\n",
        "    'rgb_to_gray_prob': config.RGB_TO_GRAY_PROB,\n",
        "    'min_jpeg_quality': config.MIN_JPEG_QUAL_AUG,\n",
        "    'max_jpeg_quality': config.MAX_JPEG_QUAL_AUG,\n",
        "    'fullbody_crop_prob': config.FULLBODY_CROP_PROB,\n",
        "    'detic_crop_prob': config.DETIC_CROP_PROB,\n",
        "    'yolov5_crop_prob': config.YOLOV5_CROP_PROB,\n",
        "    'vit_base_crop_prob': config.VIT_BASE_CROP_PROB,\n",
        "    'vit_small_crop_prob': config.VIT_SMALL_CROP_PROB,\n",
        "    'vit_moco_crop_prob': config.VIT_MOCO_CROP_PROB,\n",
        "    'non_crop_prob': config.NONE_CROP_PROB,\n",
        "    'arcface_m': config.ARC_FACE_M,\n",
        "    'emb_dim': config.EMB_DIM,\n",
        "    'gcs_ds_path': config.GCS_DS_PATH\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.695853Z",
          "iopub.status.idle": "2022-06-06T13:09:17.696436Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.696245Z",
          "shell.execute_reply": "2022-06-06T13:09:17.696269Z"
        },
        "trusted": true,
        "id": "0B7SNg2Ty2xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"happy-whale-and-dolphin\",\n",
        "    entity=\"lexkaggle\",\n",
        "    name=f'effnet-b{config.EFF_NET}-img-{config.IMAGE_SIZE}-fullbody-f-{config.FOLD_TO_RUN}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.697315Z",
          "iopub.status.idle": "2022-06-06T13:09:17.697677Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.69746Z",
          "shell.execute_reply": "2022-06-06T13:09:17.697482Z"
        },
        "trusted": true,
        "id": "92ZJk8Vwy2xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.GCS_DS_PATH\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.699399Z",
          "iopub.status.idle": "2022-06-06T13:09:17.699708Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.699553Z",
          "shell.execute_reply": "2022-06-06T13:09:17.699568Z"
        },
        "trusted": true,
        "id": "U77tXKR_y2xl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "51dfcf6d-52e8-4cd9-f703-0dbca8ebae41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happywhale-tfrecords-private2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = np.sort(np.array(tf.io.gfile.glob(config.GCS_DS_PATH + '/happywhale-2022-train*.tfrec')))\n",
        "test_files = np.sort(np.array(tf.io.gfile.glob(config.GCS_DS_PATH + '/happywhale-2022-test*.tfrec')))\n",
        "print(config.GCS_DS_PATH)\n",
        "print(len(train_files),len(test_files),count_data_items(train_files),count_data_items(test_files))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.70183Z",
          "iopub.status.idle": "2022-06-06T13:09:17.702314Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.702069Z",
          "shell.execute_reply": "2022-06-06T13:09:17.702094Z"
        },
        "trusted": true,
        "id": "zBrOvR8-y2xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cffa29e-cfb4-4dfb-a93c-7c3008326f5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happywhale-tfrecords-private2\n",
            "0 0 0.0 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_rgb_to_gray(image, probability, seed=config.SEED):\n",
        "    with tf.name_scope('RandomRGBtoGray'):\n",
        "        do_gray_random = tf.random.uniform([], seed=seed)\n",
        "\n",
        "        image = tf.cond(\n",
        "            tf.greater(do_gray_random, probability), lambda: image,\n",
        "            lambda: tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image)))\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.703798Z",
          "iopub.status.idle": "2022-06-06T13:09:17.704285Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.704036Z",
          "shell.execute_reply": "2022-06-06T13:09:17.704059Z"
        },
        "trusted": true,
        "id": "P8ydkBWRy2xl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HHYer6ysy2xl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crops = tf.convert_to_tensor(['fullbody', 'detic', 'yolov5', 'vit_base', 'vit_small', 'vit_moco', 'none'])\n",
        "\n",
        "def arcface_format(posting_id, image, label_group, species):\n",
        "    return posting_id, {'inp1': image, 'inp2': label_group, 'inp3': species}, label_group, species\n",
        "\n",
        "def arcface_inference_format(posting_id, image, label_group, species):\n",
        "    return image,posting_id\n",
        "\n",
        "def arcface_eval_format(posting_id, image, label_group, species):\n",
        "    return image,label_group\n",
        "\n",
        "def data_augment(posting_id, image, label_group, species):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_hue(image, config.RANDOM_HUE_MAX_DELTA)\n",
        "    image = tf.image.random_saturation(image, config.RANDOM_SATURATION_LOWER, config.RANDOM_SATURATION_UPPER)\n",
        "    image = tf.image.random_contrast(image, config.RANDOM_CONTRAST_LOWER, config.RANDOM_CONTRAST_UPPER)\n",
        "    image = tf.image.random_brightness(image, config.RANDOM_BRIGHTNESS_MAX_DELTA)\n",
        "    image = random_rgb_to_gray(image, config.RGB_TO_GRAY_PROB)\n",
        "    return posting_id, image, label_group, species\n",
        "\n",
        "def decode_image(image_data, bbs):\n",
        "    if bbs is not None and bbs[0] != -1:\n",
        "        left, top, right, bottom = bbs[0], bbs[1], bbs[2], bbs[3]\n",
        "        bbs = tf.convert_to_tensor([top, left, bottom - top, right - left])\n",
        "        image = tf.io.decode_and_crop_jpeg(image_data, bbs, channels=3)\n",
        "    else:\n",
        "        image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "\n",
        "    image = tf.image.resize(image, [config.IMAGE_SIZE,config.IMAGE_SIZE])\n",
        "    image = tf.clip_by_value(image, 0, 255)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example, crop_method, flip=False):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"species\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        'detic_box': tf.io.FixedLenFeature([4], tf.int64),\n",
        "        'yolov5_box': tf.io.FixedLenFeature([4], tf.int64),\n",
        "        'tc_vitbase': tf.io.FixedLenFeature([4], tf.int64),\n",
        "        'tc_vitsmall': tf.io.FixedLenFeature([4], tf.int64),\n",
        "        'tc_mocovit': tf.io.FixedLenFeature([4], tf.int64),\n",
        "        'fullbody': tf.io.FixedLenFeature([4], tf.int64)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    posting_id = example['image_name']\n",
        "    \n",
        "    if crop_method == 'random':\n",
        "        rand = tf.random.categorical(tf.math.log([[\n",
        "            config.FULLBODY_CROP_PROB,\n",
        "            config.DETIC_CROP_PROB,\n",
        "            config.YOLOV5_CROP_PROB,\n",
        "            config.VIT_BASE_CROP_PROB,\n",
        "            config.VIT_SMALL_CROP_PROB,\n",
        "            config.VIT_MOCO_CROP_PROB,\n",
        "            config.NONE_CROP_PROB\n",
        "        ]]), 1, seed=config.SEED)\n",
        "        \n",
        "        crop_method = crops[rand[0][0]]\n",
        "    \n",
        "    if crop_method == 'detic':\n",
        "        bbs = tf.cast(example['detic_box'], tf.int32)\n",
        "    elif crop_method == 'yolov5':\n",
        "        bbs = tf.cast(example['yolov5_box'], tf.int32)\n",
        "    elif crop_method == 'vit_base':\n",
        "        bbs = tf.cast(example['tc_vitbase'], tf.int32)\n",
        "    elif crop_method == 'vit_small':\n",
        "        bbs = tf.cast(example['tc_vitsmall'], tf.int32)\n",
        "    elif crop_method == 'vit_moco':\n",
        "        bbs = tf.cast(example['tc_mocovit'], tf.int32)\n",
        "    elif crop_method == 'fullbody':\n",
        "        bbs = tf.cast(example['fullbody'], tf.int32)\n",
        "    else:\n",
        "        bbs = tf.convert_to_tensor([-1, -1, -1, -1])\n",
        "\n",
        "    image = decode_image(example['image'], bbs)\n",
        "    \n",
        "    if flip:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "    \n",
        "    label_group = tf.cast(example['target'], tf.int32)\n",
        "    species = tf.cast(example['species'], tf.int32)\n",
        "    \n",
        "    return posting_id, image, label_group, species\n",
        "\n",
        "def load_dataset(filenames, ordered = False, crop_method='random', flip=False):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(lambda x: read_labeled_tfrecord(x, crop_method, flip), num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset(filenames, pseudo=False):\n",
        "    if pseudo:\n",
        "        print('Loading pseudo labels')\n",
        "        filenames = list(filenames)\n",
        "        filenames.append(config.GCS_PSEUDO_DS_PATH + '/happywhale-2022-pseudo--12157.tfrec')\n",
        "        filenames = np.array(filenames)\n",
        "    dataset = load_dataset(filenames, ordered = False)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(lambda posting_id, image, label_group, species: (image, {'metric_out': label_group, 'specie_out': species}))\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(1024)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_val_dataset(filenames):\n",
        "    dataset = load_dataset(filenames, ordered = True, crop_method='detic')\n",
        "    # dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(lambda posting_id, image, label_group, species: (image, {'metric_out': label_group, 'specie_out': species}))\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_eval_dataset(filenames, get_targets = True):\n",
        "    dataset = load_dataset(filenames, ordered = True, crop_method='detic')\n",
        "    # dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_eval_format, num_parallel_calls = AUTO)\n",
        "    if not get_targets:\n",
        "        dataset = dataset.map(lambda image, target: image)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(filenames, get_names = True, crop_method='random', flip=False):\n",
        "    dataset = load_dataset(filenames, ordered = True, crop_method=crop_method, flip=flip)\n",
        "    # dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_inference_format, num_parallel_calls = AUTO)\n",
        "    if not get_names:\n",
        "        dataset = dataset.map(lambda image, posting_id: image)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.705851Z",
          "iopub.status.idle": "2022-06-06T13:09:17.706355Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.706097Z",
          "shell.execute_reply": "2022-06-06T13:09:17.706122Z"
        },
        "trusted": true,
        "id": "2Hu8kUbry2xl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L4fT-EByy2xn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "species = json.load(open(config.LABEL_INFO_PATH / 'species.json'))\n",
        "individual_ids = json.load(open(config.LABEL_INFO_PATH / 'individual_ids.json'))\n",
        "\n",
        "id2species = {s: i for i, s in species.items()}\n",
        "id2individual_ids = {s: i for i, s in individual_ids.items()}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.707547Z",
          "iopub.status.idle": "2022-06-06T13:09:17.708038Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.707755Z",
          "shell.execute_reply": "2022-06-06T13:09:17.707779Z"
        },
        "trusted": true,
        "id": "Q5i_F0Cay2xn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a2438253-6886-4254-c552-f65c4f302914"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-731d8f0c5a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspecies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL_INFO_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'species.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindividual_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL_INFO_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'individual_ids.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mid2species\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mid2individual_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindividual_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/happywhale-generate-tfrecords-with-pseudo/species.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X08cDDQay2xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2species\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.709419Z",
          "iopub.status.idle": "2022-06-06T13:09:17.709887Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.709632Z",
          "shell.execute_reply": "2022-06-06T13:09:17.709656Z"
        },
        "trusted": true,
        "id": "9hV5qgYpy2xo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "51dcf655-a3d6-4f8d-be8d-7ae925f023a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3ba85c5f3861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid2species\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'id2species' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SPECIES = len(id2species)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.711234Z",
          "iopub.status.idle": "2022-06-06T13:09:17.711706Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.71146Z",
          "shell.execute_reply": "2022-06-06T13:09:17.711484Z"
        },
        "trusted": true,
        "id": "rQkqMlOMy2xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C6pLcUtNy2xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = 5; col = 4;\n",
        "row = min(row,config.BATCH_SIZE//col)\n",
        "N_TRAIN = count_data_items(train_files)\n",
        "print(N_TRAIN)\n",
        "ds = get_training_dataset(train_files, pseudo=True)\n",
        "\n",
        "for (sample,label) in ds:\n",
        "    img = sample['inp1']\n",
        "    plt.figure(figsize=(25,int(25*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.title(f\"{id2individual_ids[label['metric_out'][j].numpy()]} - {id2species[label['specie_out'][j].numpy()]}\")\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.712981Z",
          "iopub.status.idle": "2022-06-06T13:09:17.713446Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.713186Z",
          "shell.execute_reply": "2022-06-06T13:09:17.713209Z"
        },
        "trusted": true,
        "id": "U0Y8pIdqy2xp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "fadcb0d4-4f19-4f84-ab59-1abbc49d2b36"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "Loading pseudo labels\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2388\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m       element_spec, tensor_list)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    241\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 242\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m       element_spec, tensor_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: 'happywhale-tfrecords-pseudo-dhak/happywhale-2022-pseudo--12157.tfrec')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0f0b19bd3a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2390\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2392\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: 'happywhale-tfrecords-pseudo-dhak/happywhale-2022-pseudo--12157.tfrec')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q4lnDb09y2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = 3; col = 4;\n",
        "row = min(row,config.BATCH_SIZE//col)\n",
        "N_TEST = count_data_items(test_files)\n",
        "print(N_TEST)\n",
        "ds = get_test_dataset(test_files)\n",
        "\n",
        "for (img,label) in ds:\n",
        "    plt.figure(figsize=(25,int(25*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.title(label[j].numpy())\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.71512Z",
          "iopub.status.idle": "2022-06-06T13:09:17.715598Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.715348Z",
          "shell.execute_reply": "2022-06-06T13:09:17.715372Z"
        },
        "trusted": true,
        "id": "8Gunju3ty2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fiymDtzay2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = 3; col = 4;\n",
        "row = min(row,config.BATCH_SIZE//col)\n",
        "N_TEST = count_data_items(test_files)\n",
        "print(N_TEST)\n",
        "ds = get_test_dataset(test_files, flip=True)\n",
        "\n",
        "for (img,label) in ds:\n",
        "    plt.figure(figsize=(25,int(25*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.title(label[j].numpy())\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.716989Z",
          "iopub.status.idle": "2022-06-06T13:09:17.71733Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.717168Z",
          "shell.execute_reply": "2022-06-06T13:09:17.717184Z"
        },
        "trusted": true,
        "id": "OOLwZEway2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AQP4lebby2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified version of ArcFace from original kernel to create this. Based on paper ElasticFace: Elastic Margin Loss for Deep Face Recognition (https://arxiv.org/pdf/2109.09416.pdf).\n",
        "\n",
        "class ElasticArcFace(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes,\n",
        "        s=30,\n",
        "        mean=0.50,\n",
        "        std=0.025,\n",
        "        easy_margin=False,\n",
        "        ls_eps=0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        super(ElasticArcFace, self).__init__(**kwargs)\n",
        "        \n",
        "        print(f'ElasticArcFace mean: {mean}, s: {std}')\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.ls_eps = ls_eps\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'mean': self.mean,\n",
        "            'std': self.std,\n",
        "            'ls_eps': self.ls_eps\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ElasticArcFace, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "\n",
        "        m = tf.random.normal((tf.shape(y)[0], 1), mean=self.mean, stddev=self.std, seed=config.SEED)\n",
        "\n",
        "        cos_m = tf.math.cos(m)\n",
        "        sin_m = tf.math.sin(m)\n",
        "        th = tf.math.cos(math.pi - m)\n",
        "        mm = tf.math.sin(math.pi - m) * m\n",
        "        \n",
        "        phi = cosine * cos_m - sine * sin_m\n",
        "\n",
        "        phi = tf.where(cosine > th, phi, cosine - mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "            )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.7188Z",
          "iopub.status.idle": "2022-06-06T13:09:17.71915Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.718972Z",
          "shell.execute_reply": "2022-06-06T13:09:17.718993Z"
        },
        "trusted": true,
        "id": "uD98f1wHy2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6svZrdcMy2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
        "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
        "\n",
        "def freeze_BN(model):\n",
        "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "def get_model():    \n",
        "    with strategy.scope():\n",
        "        margin = ElasticArcFace(\n",
        "            n_classes = config.N_CLASSES, \n",
        "            s = 30, \n",
        "            mean = config.ARC_FACE_M,\n",
        "            std=0.025,\n",
        "            name=f'head/{config.head}', \n",
        "            dtype='float32'\n",
        "        )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = [config.IMAGE_SIZE, config.IMAGE_SIZE, 3], name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "    \n",
        "        species = tf.keras.layers.Input(shape = (), name = 'inp3')\n",
        "        \n",
        "        if config.model_type == 'effnetv1':\n",
        "            x = EFNS[config.EFF_NET](weights = 'noisy-student', include_top = False)(inp)\n",
        "            # Concat pooling\n",
        "            avg_pool = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "            max_pool = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "            pretrained_out = tf.keras.layers.Concatenate()([avg_pool, max_pool])\n",
        "        elif config.model_type == 'effnetv2':\n",
        "            FEATURE_VECTOR = f'{EFFNETV2_ROOT}/tfhub_models/efficientnetv2-{config.EFF_NETV2}/feature_vector'\n",
        "            embed = tfhub.KerasLayer(FEATURE_VECTOR, trainable=True)(inp)\n",
        "        \n",
        "        # Do specie classification.\n",
        "        specie_in = tf.keras.layers.Dropout(0.2)(pretrained_out)\n",
        "        specie_out = tf.keras.layers.Dense(2048)(specie_in)\n",
        "        specie_out = tf.keras.layers.Dropout(0.2)(specie_out)\n",
        "        specie_out = tf.keras.layers.Dense(NUM_SPECIES, activation='softmax', name='specie_out')(specie_out)\n",
        "\n",
        "        # Do metric\n",
        "        print(f'Size of embed {config.EMB_DIM}')\n",
        "        pre_margin_dense_layer = tf.keras.layers.Dense(config.EMB_DIM)\n",
        "    \n",
        "        # Multiple-sample dropout https://arxiv.org/abs/1905.09788\n",
        "        dropout_base = 0.17\n",
        "        drop_ls = [tf.keras.layers.Dropout((dropout_base + 0.01), seed=420),\n",
        "                   tf.keras.layers.Dropout((dropout_base + 0.02), seed=4200),\n",
        "                   tf.keras.layers.Dropout((dropout_base + 0.03), seed=42000),\n",
        "                   tf.keras.layers.Dropout((dropout_base + 0.04), seed=420000),\n",
        "                   tf.keras.layers.Dropout((dropout_base + 0.05), seed=4200000)]\n",
        "\n",
        "        for ii, drop in enumerate(drop_ls):\n",
        "            if ii == 0:\n",
        "                embed = (pre_margin_dense_layer(drop(pretrained_out)) / 5.0)\n",
        "            else:\n",
        "                embed += (pre_margin_dense_layer(drop(pretrained_out)) / 5.0)\n",
        "                \n",
        "        embed = tf.keras.layers.BatchNormalization()(embed)\n",
        "        embed = tf.math.l2_normalize(embed, axis=1)\n",
        "        \n",
        "        x = margin([embed, label])\n",
        "        output = tf.keras.layers.Softmax(dtype='float32', name='metric_out')(x)\n",
        "        \n",
        "        model = tf.keras.models.Model(inputs = [inp, label, species], outputs = [output, specie_out])\n",
        "        embed_model = tf.keras.models.Model(inputs = inp, outputs = embed)  \n",
        "        \n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = {\n",
        "                'metric_out': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                'specie_out': tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "            },\n",
        "            loss_weights = {\n",
        "                'metric_out': config.WEIGHT_METRIC,\n",
        "                'specie_out': config.WEIGHT_SPECIES\n",
        "            },\n",
        "            metrics = {\n",
        "                'metric_out': [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)],\n",
        "                'specie_out': tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "            }\n",
        "        ) \n",
        "        \n",
        "        return model,embed_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.720274Z",
          "iopub.status.idle": "2022-06-06T13:09:17.720584Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.720428Z",
          "shell.execute_reply": "2022-06-06T13:09:17.720443Z"
        },
        "trusted": true,
        "id": "Ms1b96auy2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr_callback(plot=False):\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * config.BATCH_SIZE  \n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 4\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.9\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if config.RESUME:\n",
        "            epoch = epoch + config.RESUME_EPOCH\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "            \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "            \n",
        "        return lr\n",
        "        \n",
        "    if plot:\n",
        "        epochs = list(range(config.EPOCHS))\n",
        "        learning_rates = [lrfn(x) for x in epochs]\n",
        "        print([f'{i:.20f}' for i in learning_rates])\n",
        "        plt.scatter(epochs,learning_rates)\n",
        "        plt.show()\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
        "    return lr_callback\n",
        "\n",
        "get_lr_callback(plot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.721924Z",
          "iopub.status.idle": "2022-06-06T13:09:17.722576Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.722366Z",
          "shell.execute_reply": "2022-06-06T13:09:17.722394Z"
        },
        "trusted": true,
        "id": "aOjY2Lany2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ukpQTx3y2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Snapshot(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self,fold,snapshot_epochs=[]):\n",
        "        super(Snapshot, self).__init__()\n",
        "        self.snapshot_epochs = snapshot_epochs\n",
        "        self.fold = fold\n",
        "        \n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_weights(config.save_dir+f\"/{config.MODEL_NAME}_last.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.723945Z",
          "iopub.status.idle": "2022-06-06T13:09:17.72431Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.724131Z",
          "shell.execute_reply": "2022-06-06T13:09:17.724155Z"
        },
        "trusted": true,
        "id": "K96cU-Yky2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ukGfJc1qy2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS!=config.FOLD_TO_RUN]\n",
        "VALIDATION_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS==config.FOLD_TO_RUN]\n",
        "print(len(TRAINING_FILENAMES),len(VALIDATION_FILENAMES),count_data_items(TRAINING_FILENAMES),count_data_items(VALIDATION_FILENAMES))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.7253Z",
          "iopub.status.idle": "2022-06-06T13:09:17.726048Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.725762Z",
          "shell.execute_reply": "2022-06-06T13:09:17.725785Z"
        },
        "trusted": true,
        "id": "2G4n_J1ky2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VQenzsFQy2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(config.SEED)\n",
        "VERBOSE = 1\n",
        "train_dataset = get_training_dataset(TRAINING_FILENAMES, pseudo=True)\n",
        "val_dataset = None\n",
        "if config.FOLD_TO_RUN is not None:\n",
        "    val_dataset = get_val_dataset(VALIDATION_FILENAMES)\n",
        "else:\n",
        "    print('No val dataset')\n",
        "\n",
        "num_pseudo = 12157\n",
        "STEPS_PER_EPOCH = (count_data_items(TRAINING_FILENAMES) + num_pseudo) // config.BATCH_SIZE\n",
        "\n",
        "# SAVE BEST MODEL EACH FOLD        \n",
        "sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "    OUTPUT_DIR / f'{config.MODEL_NAME}_loss.h5',\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    save_freq='epoch')\n",
        "\n",
        "# BUILD MODEL\n",
        "K.clear_session()\n",
        "model,embed_model = get_model()\n",
        "snap = Snapshot(fold=config.FOLD_TO_RUN)\n",
        "model.summary()\n",
        "\n",
        "callbacks = [get_lr_callback(), snap]\n",
        "if config.FOLD_TO_RUN is not None:\n",
        "    callbacks.append(sv_loss)\n",
        "\n",
        "if config.LOAD_WEIGHTS:\n",
        "    print('Loading weights  ' + config.LOAD_WEIGHTS)\n",
        "    model.load_weights(config.LOAD_WEIGHTS)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.729411Z",
          "iopub.execute_input": "2022-06-06T13:09:17.729633Z",
          "iopub.status.idle": "2022-06-06T13:09:17.750094Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.729607Z",
          "shell.execute_reply": "2022-06-06T13:09:17.748466Z"
        },
        "trusted": true,
        "id": "J9n19iRey2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config.EPOCHS:\n",
        "    print('#### Image Size %i with EfficientNet B%i and batch_size %i'%\n",
        "          (config.IMAGE_SIZE,config.EFF_NET,config.BATCH_SIZE))\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data = val_dataset,\n",
        "        steps_per_epoch = STEPS_PER_EPOCH,\n",
        "        epochs = config.EPOCHS,\n",
        "        callbacks = callbacks,\n",
        "        verbose = VERBOSE)\n",
        "\n",
        "    if config.FOLD_TO_RUN is not None:\n",
        "        model.load_weights(OUTPUT_DIR / f'{config.MODEL_NAME}_loss.h5')\n",
        "\n",
        "    try:\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print('No training.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.750995Z",
          "iopub.status.idle": "2022-06-06T13:09:17.75172Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.751507Z",
          "shell.execute_reply": "2022-06-06T13:09:17.751533Z"
        },
        "trusted": true,
        "id": "JDjvMryiy2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cKdmcCoey2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids(filename):\n",
        "    ds = get_test_dataset([filename],get_names=True).map(lambda image, image_name: image_name).unbatch()\n",
        "    NUM_IMAGES = count_data_items([filename])\n",
        "    ids = next(iter(ds.batch(NUM_IMAGES))).numpy().astype('U')\n",
        "    return ids\n",
        "\n",
        "def get_targets(filename):\n",
        "    ds = get_eval_dataset([filename],get_targets=True).map(lambda image, target: target).unbatch()\n",
        "    NUM_IMAGES = count_data_items([filename])\n",
        "    ids = next(iter(ds.batch(NUM_IMAGES))).numpy()\n",
        "    return ids\n",
        "\n",
        "def get_embeddings(filename, crop_method, flip=False):\n",
        "    ds = get_test_dataset([filename],get_names=False, crop_method=crop_method, flip=flip)\n",
        "    embeddings = embed_model.predict(ds,verbose=0)\n",
        "    return embeddings\n",
        "\n",
        "def get_predictions(test_df,threshold=0.2):\n",
        "    predictions = {}\n",
        "    for i,row in tqdm(test_df.iterrows()):\n",
        "        if row.image in predictions:\n",
        "            if len(predictions[row.image])==5:\n",
        "                continue\n",
        "            predictions[row.image].append(row.target)\n",
        "        elif row.confidence>threshold:\n",
        "            predictions[row.image] = [row.target,'new_individual']\n",
        "        else:\n",
        "            predictions[row.image] = ['new_individual',row.target]\n",
        "\n",
        "    for x in tqdm(predictions):\n",
        "        if len(predictions[x])<5:\n",
        "            remaining = [y for y in sample_list if y not in predictions]\n",
        "            predictions[x] = predictions[x]+remaining\n",
        "            predictions[x] = predictions[x][:5]\n",
        "        \n",
        "    return predictions\n",
        "\n",
        "def map_per_image(label, predictions):\n",
        "    \"\"\"Computes the precision score of one image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    label : string\n",
        "            The true label of the image\n",
        "    predictions : list\n",
        "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "    \"\"\"    \n",
        "    try:\n",
        "        return 1 / (predictions[:5].index(label) + 1)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "    \n",
        "f = open (config.LABEL_INFO_PATH / 'individual_ids.json', \"r\")\n",
        "target_encodings = json.loads(f.read())\n",
        "target_encodings = {target_encodings[x]:x for x in target_encodings}\n",
        "\n",
        "sample_list = [\n",
        "    '938b7e931166', 'ca69a5d7c122', '18efa8d0b472', '91ed5caeb0d3', '7362d7a01d00',\n",
        "    'ae4343270756', '8e5253662392', '0b180ad0afa2', '6a6fa3ec3810', '6a3af6e0c55c'\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.753164Z",
          "iopub.status.idle": "2022-06-06T13:09:17.753655Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.753481Z",
          "shell.execute_reply": "2022-06-06T13:09:17.7535Z"
        },
        "trusted": true,
        "id": "OJjQ_ntey2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kMlOiSIly2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_for_filenames(filenames, crop_method, flip=False):\n",
        "    output = []\n",
        "    for filename in tqdm(filenames):\n",
        "        embeddings = get_embeddings(filename, crop_method, flip=flip)\n",
        "        output.append(embeddings)\n",
        "        \n",
        "    return np.concatenate(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.754639Z",
          "iopub.status.idle": "2022-06-06T13:09:17.755172Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.754901Z",
          "shell.execute_reply": "2022-06-06T13:09:17.754921Z"
        },
        "trusted": true,
        "id": "-lDVoJZyy2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VLBLY_TZy2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_all_embeddings(filenames, prefix):\n",
        "    # This will be lazily set by the first generated embedding, as only then will we know the dimensions.\n",
        "    all_embeddings = None\n",
        "\n",
        "    crop_weights = config.CROP_WEIGHTS\n",
        "    total_weight = sum(crop_weights.values())\n",
        "    crop_weights_adjusted = {k: (v / total_weight) for (k, v) in crop_weights.items()}\n",
        "    \n",
        "    i = 0\n",
        "    for crop_method, weight in crop_weights_adjusted.items():\n",
        "        print(f'Do {crop_method} no flip')\n",
        "        train_embeddings_no_flip = get_embeddings_for_filenames(filenames, crop_method)\n",
        "        \n",
        "        print(f'Do {crop_method} flip')\n",
        "        train_embeddings_flip = get_embeddings_for_filenames(filenames, crop_method, flip=True)\n",
        "        \n",
        "        emb_arr = np.mean(np.stack([train_embeddings_no_flip, train_embeddings_flip]), axis=0)\n",
        "        #fh = open(OUTPUT_DIR / f'{crop_method}_{prefix}_embeddings_fold{config.FOLD_TO_RUN or \"\"}.npz', 'wb')\n",
        "        #np.save(fh, emb_arr)\n",
        "        \n",
        "        if all_embeddings is None:\n",
        "            all_embeddings = np.zeros((len(crop_weights_adjusted), emb_arr.shape[0], emb_arr.shape[1]))\n",
        "            print(all_embeddings.shape)\n",
        "    \n",
        "        all_embeddings[i] = emb_arr * weight\n",
        "        i += 1\n",
        "    \n",
        "    # Try different style of merging embeddings in future.\n",
        "    mean = np.mean(all_embeddings, axis=0)\n",
        "    fh = open(OUTPUT_DIR / f'{prefix}_mean_embeddings_fold{config.FOLD_TO_RUN or \"\"}.npz', 'wb')\n",
        "    np.save(fh, mean)\n",
        "    \n",
        "    return mean"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.756127Z",
          "iopub.status.idle": "2022-06-06T13:09:17.756571Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.756379Z",
          "shell.execute_reply": "2022-06-06T13:09:17.756395Z"
        },
        "trusted": true,
        "id": "fp_5IUWsy2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I9iIjk42y2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ids(filenames, prefix, include_targets=False):\n",
        "    train_targets = []\n",
        "    train_ids = []\n",
        "    for filename in tqdm(filenames):\n",
        "        if include_targets:\n",
        "            targets = get_targets(filename)\n",
        "            train_targets.append(targets)\n",
        "    \n",
        "        ids = get_ids(filename)\n",
        "        train_ids.append(ids)\n",
        "\n",
        "    if include_targets:\n",
        "        train_targets = np.concatenate(train_targets)\n",
        "        targets_fh = open(OUTPUT_DIR / f'{prefix}_targets_fold{config.FOLD_TO_RUN or \"\"}.npz', 'wb')\n",
        "        np.save(targets_fh, train_targets)\n",
        "\n",
        "    train_ids = np.concatenate(train_ids)\n",
        "    ids_fh = open(OUTPUT_DIR / f'{prefix}_ids_fold{config.FOLD_TO_RUN or \"\"}.npz', 'wb')\n",
        "    np.save(ids_fh, train_ids)\n",
        "    \n",
        "    return train_ids, train_targets if include_targets else None\n",
        "\n",
        "def load_all_embeddings(path, embed_name):\n",
        "    return np.load(open(path / f'{embed_name}_mean_embeddings_fold.npz', 'rb'))\n",
        "\n",
        "def get_ids(path, prefix):\n",
        "    return np.load(open(path / f'{prefix}_ids_fold.npz', 'rb'))\n",
        "\n",
        "def get_targets(path, prefix):\n",
        "    return np.load(open(path / f'{prefix}_targets_fold.npz', 'rb'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.757749Z",
          "iopub.status.idle": "2022-06-06T13:09:17.758119Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.757915Z",
          "shell.execute_reply": "2022-06-06T13:09:17.757936Z"
        },
        "trusted": true,
        "id": "uRIRBwcYy2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-itBvTQCy2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.759044Z",
          "iopub.status.idle": "2022-06-06T13:09:17.759372Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.759191Z",
          "shell.execute_reply": "2022-06-06T13:09:17.759213Z"
        },
        "trusted": true,
        "id": "c5TCGrony2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n",
        "# Remove the if False if training in Kaggle.\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.76104Z",
          "iopub.status.idle": "2022-06-06T13:09:17.761383Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.7612Z",
          "shell.execute_reply": "2022-06-06T13:09:17.761222Z"
        },
        "trusted": true,
        "id": "JTxiIdG2y2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the if False if training in Kaggle.\n",
        "is_training = False\n",
        "if is_training:\n",
        "    TRAINING_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS!=config.VAL_FOLD_TO_RUN]\n",
        "    VALIDATION_FILENAMES = [x for i,x in enumerate(train_files) if i%config.FOLDS==config.VAL_FOLD_TO_RUN]\n",
        "\n",
        "    print(TRAINING_FILENAMES, VALIDATION_FILENAMES)\n",
        "\n",
        "    train_embeddings = prepare_all_embeddings(TRAINING_FILENAMES, 'train')\n",
        "    val_embeddings = prepare_all_embeddings(VALIDATION_FILENAMES, 'val')\n",
        "    test_embeddings = prepare_all_embeddings(test_files, 'test')\n",
        "\n",
        "    train_ids, train_targets = save_ids(TRAINING_FILENAMES, 'train', include_targets=True)\n",
        "    print(train_targets.shape)\n",
        "\n",
        "    val_ids, val_targets = save_ids(VALIDATION_FILENAMES, 'val', include_targets=True)\n",
        "    print(val_targets.shape)\n",
        "\n",
        "    test_ids, _ = save_ids(test_files, 'test')\n",
        "else:\n",
        "    embed_path = Path('../input/happywhale-download-b5-with-dhak-pseudo-multisam/b5-with-dhak-pseudo-multisampledropout')\n",
        "\n",
        "    train_embeddings = load_all_embeddings(embed_path, 'train')\n",
        "    val_embeddings = load_all_embeddings(embed_path, 'val')\n",
        "    test_embeddings = load_all_embeddings(embed_path, 'test')\n",
        "\n",
        "    train_ids = get_ids(embed_path, 'train')\n",
        "    train_targets = get_targets(embed_path, 'train')\n",
        "\n",
        "    val_ids = get_ids(embed_path, 'val')\n",
        "    val_targets = get_targets(embed_path, 'val')\n",
        "\n",
        "    test_ids = get_ids(embed_path, 'test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.762872Z",
          "iopub.status.idle": "2022-06-06T13:09:17.763246Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.763066Z",
          "shell.execute_reply": "2022-06-06T13:09:17.763089Z"
        },
        "trusted": true,
        "id": "TKqBGS-by2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DEs7ioL2y2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.764096Z",
          "iopub.status.idle": "2022-06-06T13:09:17.764416Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.764244Z",
          "shell.execute_reply": "2022-06-06T13:09:17.764266Z"
        },
        "trusted": true,
        "id": "RPAAH7KTy2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c8PhJpCNy2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_threshold_adjusted = 0.6\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=config.KNN,metric='cosine')\n",
        "neigh.fit(train_embeddings)\n",
        "val_nn_distances, val_nn_idxs = neigh.kneighbors(val_embeddings, config.KNN, return_distance=True)\n",
        "allowed_targets = set([target_encodings[x] for x in np.unique(train_targets)])\n",
        "val_targets_df = pd.DataFrame(np.stack([val_ids,val_targets],axis=1),columns=['image','target'])\n",
        "val_targets_df['target'] = val_targets_df['target'].astype(int).map(target_encodings)\n",
        "val_targets_df.loc[~val_targets_df.target.isin(allowed_targets),'target'] = 'new_individual'\n",
        "val_targets_df.target.value_counts()\n",
        "val_df = []\n",
        "for i in tqdm(range(len(val_ids))):\n",
        "    id_ = val_ids[i]\n",
        "    targets = train_targets[val_nn_idxs[i]]\n",
        "    distances = val_nn_distances[i]\n",
        "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
        "    subset_preds['image'] = id_\n",
        "    val_df.append(subset_preds)\n",
        "val_df = pd.concat(val_df).reset_index(drop=True)\n",
        "val_df['confidence'] = 1-val_df['distances']\n",
        "val_df = val_df.groupby(['image','target']).confidence.max().reset_index()\n",
        "val_df = val_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
        "val_df['target'] = val_df['target'].map(target_encodings)\n",
        "val_df.to_csv('val_neighbors.csv')\n",
        "val_df.image.value_counts().value_counts()\n",
        "\n",
        "## Compute CV\n",
        "th = 0.6\n",
        "cv = 0\n",
        "\n",
        "all_preds = get_predictions(val_df,threshold=th)\n",
        "for i,row in val_targets_df.iterrows():\n",
        "    target = row.target\n",
        "    preds = all_preds[row.image]\n",
        "    val_targets_df.loc[i,th] = map_per_image(target,preds)\n",
        "cv = val_targets_df[th].mean()\n",
        "print(f\"CV at threshold {th}: {cv}\")\n",
        "\n",
        "val_targets_df.describe()\n",
        "\n",
        "if config.FOLD_TO_RUN == config.VAL_FOLD_TO_RUN:\n",
        "    wandb.log({\"best_cv\": best_cv, \"best_threshold\": best_th})\n",
        "\n",
        "## Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
        "val_targets_df['is_new_individual'] = val_targets_df.target=='new_individual'\n",
        "print(val_targets_df.is_new_individual.value_counts().to_dict())\n",
        "val_scores = val_targets_df.groupby('is_new_individual').mean().T\n",
        "val_scores['adjusted_cv'] = val_scores[True]*0.1+val_scores[False]*0.9\n",
        "best_threshold_adjusted = val_scores['adjusted_cv'].idxmax()\n",
        "print(\"best_threshold\",best_threshold_adjusted)\n",
        "val_scores\n",
        "\n",
        "train_embeddings = np.concatenate([train_embeddings,val_embeddings])\n",
        "train_targets = np.concatenate([train_targets,val_targets])\n",
        "print(train_embeddings.shape,train_targets.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.766025Z",
          "iopub.status.idle": "2022-06-06T13:09:17.766492Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.766274Z",
          "shell.execute_reply": "2022-06-06T13:09:17.76629Z"
        },
        "trusted": true,
        "id": "AHlZ2cory2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "neigh = NearestNeighbors(n_neighbors=config.KNN,metric='cosine')\n",
        "neigh.fit(train_embeddings)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.767589Z",
          "iopub.status.idle": "2022-06-06T13:09:17.768133Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.767832Z",
          "shell.execute_reply": "2022-06-06T13:09:17.767867Z"
        },
        "trusted": true,
        "id": "fhnEV-Igy2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_nn_distances, test_nn_idxs = neigh.kneighbors(test_embeddings, config.KNN, return_distance=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.769056Z",
          "iopub.status.idle": "2022-06-06T13:09:17.769515Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.769297Z",
          "shell.execute_reply": "2022-06-06T13:09:17.769312Z"
        },
        "trusted": true,
        "id": "3NDaD01Ly2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(config.INPUT_DATA_PATH / 'sample_submission.csv',index_col='image')\n",
        "print(len(test_ids),len(sample_submission))\n",
        "test_df = []\n",
        "for i in tqdm(range(len(test_ids))):\n",
        "    id_ = test_ids[i]\n",
        "    targets = train_targets[test_nn_idxs[i]]\n",
        "    distances = test_nn_distances[i]\n",
        "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
        "    subset_preds['image'] = id_\n",
        "    test_df.append(subset_preds)\n",
        "test_df = pd.concat(test_df).reset_index(drop=True)\n",
        "test_df['confidence'] = 1-test_df['distances']\n",
        "test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n",
        "test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
        "test_df['target'] = test_df['target'].map(target_encodings)\n",
        "test_df.to_csv('test_neighbors.csv')\n",
        "test_df.image.value_counts().value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.770397Z",
          "iopub.status.idle": "2022-06-06T13:09:17.770918Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.770672Z",
          "shell.execute_reply": "2022-06-06T13:09:17.770688Z"
        },
        "trusted": true,
        "id": "mtCGtPEBy2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.771828Z",
          "iopub.status.idle": "2022-06-06T13:09:17.772368Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.772104Z",
          "shell.execute_reply": "2022-06-06T13:09:17.772152Z"
        },
        "trusted": true,
        "id": "LpuO6Iwby2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = {}\n",
        "for i,row in tqdm(test_df.iterrows()):\n",
        "    if row.image in predictions:\n",
        "        if len(predictions[row.image])==5:\n",
        "            continue\n",
        "        predictions[row.image].append(row.target)\n",
        "    elif row.confidence>best_threshold_adjusted:\n",
        "        predictions[row.image] = [row.target,'new_individual']\n",
        "    else:\n",
        "        predictions[row.image] = ['new_individual',row.target]\n",
        "        \n",
        "for x in tqdm(predictions):\n",
        "    if len(predictions[x])<5:\n",
        "        remaining = [y for y in sample_list if y not in predictions]\n",
        "        predictions[x] = predictions[x]+remaining\n",
        "        predictions[x] = predictions[x][:5]\n",
        "    predictions[x] = ' '.join(predictions[x])\n",
        "    \n",
        "predictions = pd.Series(predictions).reset_index()\n",
        "predictions.columns = ['image','predictions']\n",
        "predictions.to_csv('submission.csv',index=False)\n",
        "predictions.head()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-06T13:09:17.773141Z",
          "iopub.status.idle": "2022-06-06T13:09:17.773472Z",
          "shell.execute_reply.started": "2022-06-06T13:09:17.773288Z",
          "shell.execute_reply": "2022-06-06T13:09:17.77331Z"
        },
        "trusted": true,
        "id": "ezHL6nQ_y2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ve-eh-rby2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xdMaq9Hvy2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fvaD-9Tay2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KlMEIh6ky2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o76_8HUsy2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "brwKJ6Eyy2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sT4VrmmJy2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TNaHsY7Ly2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "srdZr3dyy2xu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}